{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "bioasq_Question_answering_with_bert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedovich19/Machine-Learning-Projects/blob/master/bioasq_Question_answering_with_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82tjataBDKJC",
        "outputId": "86c841fb-afa7-4ebe-b389-d58c77a20f50"
      },
      "source": [
        "# Transformers installation\n",
        "! pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/91/61d69d58a1af1bd81d9ca9d62c90a6de3ab80d77f27c5df65d9a2c1f5626/transformers-4.5.0-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 20.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 52.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 52.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=3c143320ac3789a73a23e72afd52875f5d0fd983ca30b59513a5685d0a654e6f\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn4SGwBImQuI"
      },
      "source": [
        "import json\n",
        "import pandas as pd \n",
        "import collections\n",
        "import numpy as np\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from termcolor import colored\n",
        "import textwrap\n",
        "from transformers import (\n",
        "    XLNetTokenizerFast,\n",
        "    XLNetForQuestionAnswering,\n",
        "    DistilBertTokenizerFast,\n",
        "    DistilBertForQuestionAnswering,\n",
        "    BertTokenizerFast,\n",
        "    BertForQuestionAnswering,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    default_data_collator\n",
        ")\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5fz6mNDDK9l",
        "outputId": "cd35f17d-de95-4a9e-d741-453dd8d2941b"
      },
      "source": [
        "!gdown --id 19ft5q44W4SuptJgTwR84xZjsHg1jvjSZ\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19ft5q44W4SuptJgTwR84xZjsHg1jvjSZ\n",
            "To: /content/QA.zip\n",
            "\r0.00B [00:00, ?B/s]\r5.48MB [00:00, 48.2MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Swy0ACLy8siu"
      },
      "source": [
        "!unzip -q QA.zip"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9ZHLLfh8vsu"
      },
      "source": [
        "def extract_questions_and_answers(factoid_path: Path):\n",
        "  with factoid_path.open() as json_file:\n",
        "    data = json.load(json_file)\n",
        "  \n",
        "  questions = data['data'][0]['paragraphs']\n",
        "\n",
        "  data_rows = []\n",
        "\n",
        "  for question in questions:\n",
        "    context = question['context']\n",
        "    for question_and_answers in question['qas']:\n",
        "      question = question_and_answers['question']\n",
        "      id = question_and_answers['id']\n",
        "      answers = question_and_answers['answers']\n",
        "      for answer in answers:\n",
        "        answer_text = answer['text']\n",
        "        answer_start = answer['answer_start']\n",
        "        answer_end  = answer_start + len(answer_text)\n",
        "\n",
        "        data_rows.append({\n",
        "            'id':id,\n",
        "            'question': question,\n",
        "            'context' : context,\n",
        "            \"answer_text\" : answer_text,\n",
        "            \"answer_start\" : answer_start,\n",
        "            \"answer_end\" : answer_end\n",
        "        })\n",
        "  return pd.DataFrame(data_rows)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxY-ccsr80Zj",
        "outputId": "66416816-7b05-4fe1-df85-49e508279a72"
      },
      "source": [
        "factoid_paths = sorted(list(Path(\"BioASQ/\").glob(\"BioASQ-train-*\")))\n",
        "factoid_paths"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('BioASQ/BioASQ-train-factoid-4b.json'),\n",
              " PosixPath('BioASQ/BioASQ-train-factoid-5b.json'),\n",
              " PosixPath('BioASQ/BioASQ-train-factoid-6b.json')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBeMiMyP82kj"
      },
      "source": [
        "dfs = []\n",
        "for factoid_path in factoid_paths:\n",
        "  dfs.append(extract_questions_and_answers(factoid_path))\n",
        "df = pd.concat(dfs)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 839
        },
        "id": "KZNU2HI589O4",
        "outputId": "60f60394-bf3f-4782-cd1e-dccb97da6da5"
      },
      "source": [
        "df"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>answer_text</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>answer_end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>52bf208003868f1b06000019_002</td>\n",
              "      <td>What is the inheritance pattern of Li–Fraumeni...</td>\n",
              "      <td>Balanced t(11;15)(q23;q15) in a TP53+/+ breast...</td>\n",
              "      <td>autosomal dominant</td>\n",
              "      <td>213</td>\n",
              "      <td>231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>52bf208003868f1b06000019_003</td>\n",
              "      <td>What is the inheritance pattern of Li–Fraumeni...</td>\n",
              "      <td>Genetic modeling of Li-Fraumeni syndrome in ze...</td>\n",
              "      <td>autosomal dominant</td>\n",
              "      <td>105</td>\n",
              "      <td>123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>530cf4fe960c95ad0c00000b_001</td>\n",
              "      <td>Which type of lung cancer is afatinib used for?</td>\n",
              "      <td>Clinical perspective of afatinib in non-small ...</td>\n",
              "      <td>EGFR-mutant NSCLC</td>\n",
              "      <td>1203</td>\n",
              "      <td>1220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53148a07dae131f847000002_001</td>\n",
              "      <td>Which hormone abnormalities are characteristic...</td>\n",
              "      <td>DOCA sensitive pendrin expression in kidney, h...</td>\n",
              "      <td>thyroid</td>\n",
              "      <td>419</td>\n",
              "      <td>426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>53148a07dae131f847000002_002</td>\n",
              "      <td>Which hormone abnormalities are characteristic...</td>\n",
              "      <td>Clinical and molecular characteristics of Pend...</td>\n",
              "      <td>thyroid</td>\n",
              "      <td>705</td>\n",
              "      <td>712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4767</th>\n",
              "      <td>58dcb47c8acda34529000020_002</td>\n",
              "      <td>What is the role of TAD protein domain?</td>\n",
              "      <td>Sequestration of p53 in the cytoplasm by adeno...</td>\n",
              "      <td>transactivation domain</td>\n",
              "      <td>765</td>\n",
              "      <td>787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4768</th>\n",
              "      <td>58dcb47c8acda34529000020_003</td>\n",
              "      <td>What is the role of TAD protein domain?</td>\n",
              "      <td>Leu628 of the KIX domain of CBP is a key resid...</td>\n",
              "      <td>transactivation domain</td>\n",
              "      <td>139</td>\n",
              "      <td>161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4769</th>\n",
              "      <td>58dcb47c8acda34529000020_004</td>\n",
              "      <td>What is the role of TAD protein domain?</td>\n",
              "      <td>Sequestration of p53 in the cytoplasm by adeno...</td>\n",
              "      <td>transactivation domain</td>\n",
              "      <td>765</td>\n",
              "      <td>787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4770</th>\n",
              "      <td>58dcb47c8acda34529000020_005</td>\n",
              "      <td>What is the role of TAD protein domain?</td>\n",
              "      <td>Essential roles of Da transactivation domains ...</td>\n",
              "      <td>transcription activation domain</td>\n",
              "      <td>401</td>\n",
              "      <td>432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4771</th>\n",
              "      <td>58dcb47c8acda34529000020_006</td>\n",
              "      <td>What is the role of TAD protein domain?</td>\n",
              "      <td>Histone deacetylase inhibitors synergize p300 ...</td>\n",
              "      <td>transactivation domain</td>\n",
              "      <td>498</td>\n",
              "      <td>520</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12988 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                id  ... answer_end\n",
              "0     52bf208003868f1b06000019_002  ...        231\n",
              "1     52bf208003868f1b06000019_003  ...        123\n",
              "2     530cf4fe960c95ad0c00000b_001  ...       1220\n",
              "3     53148a07dae131f847000002_001  ...        426\n",
              "4     53148a07dae131f847000002_002  ...        712\n",
              "...                            ...  ...        ...\n",
              "4767  58dcb47c8acda34529000020_002  ...        787\n",
              "4768  58dcb47c8acda34529000020_003  ...        161\n",
              "4769  58dcb47c8acda34529000020_004  ...        787\n",
              "4770  58dcb47c8acda34529000020_005  ...        432\n",
              "4771  58dcb47c8acda34529000020_006  ...        520\n",
              "\n",
              "[12988 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHuFkSgoykkz"
      },
      "source": [
        "df = df[df['answer_start']>=0]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUHtJtOb9Bps",
        "outputId": "2c706c40-e66a-4f2a-bd74-ed70b8a7a5c6"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12984, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVYVZXCW9Q_R"
      },
      "source": [
        "train_df,val_df = train_test_split(df,test_size=0.05)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRFt9ksMvfGK",
        "outputId": "100c8a7f-3856-485b-8c54-94cb83901825"
      },
      "source": [
        "val_df.iloc[0]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                                   56c1f00cef6e39474100003e_003\n",
              "question                Aleglitazar is agonist of which receptor?\n",
              "context         Effects of the dual peroxisome proliferator-ac...\n",
              "answer_text        peroxisome proliferator-activated receptor-α/γ\n",
              "answer_start                                                   20\n",
              "answer_end                                                     66\n",
              "Name: 3586, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYzRaU939Rk_",
        "outputId": "2b30b916-be58-4199-d6bc-adf69aeef558"
      },
      "source": [
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((12334, 6), (650, 6))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBGz4efh-VLT"
      },
      "source": [
        "train_df_new = train_df.to_dict('list')\n",
        "val_df_new = val_df.to_dict('list')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Rb6B8Mj-je9",
        "outputId": "db1d371a-7a1e-44c1-c275-8da93c5728e1"
      },
      "source": [
        "train_df_new.keys()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['id', 'question', 'context', 'answer_text', 'answer_start', 'answer_end'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERvKuemwDKJV"
      },
      "source": [
        "# Fine-tuning with custom datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKWNBGZpB3Xd"
      },
      "source": [
        "model_checkpoint = 'bert-base-uncased'"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEk3WW2q_Ir0"
      },
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained(model_checkpoint)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11u0pdsgDKKs"
      },
      "source": [
        "Now `train_answers` and `val_answers` include the character end positions and the corrected start positions. Next,\n",
        "let's tokenize our context/question pairs. 🤗 Tokenizers can accept parallel lists of sequences and encode them together\n",
        "as sequence pairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vcs2a5rsTek"
      },
      "source": [
        "max_length = 400 # The maximum length of a feature (question and context)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH2no6iVDKKt"
      },
      "source": [
        "train_encodings = tokenizer(train_df_new['context'], train_df_new['question'], truncation=True, padding='max_length',max_length=400)\n",
        "val_encodings = tokenizer(val_df_new['context'], val_df_new['question'], truncation=True,  padding='max_length',max_length=400)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjdp9ZRMDKKt"
      },
      "source": [
        "Next we need to convert our character start/end positions to token start/end positions. When using 🤗 Fast Tokenizers,\n",
        "we can use the built in `BatchEncoding.char_to_token` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZoeKLRiDKKu"
      },
      "source": [
        "def add_token_positions(encodings, df):\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    for i in range(len(df['answer_text'])):\n",
        "        start_positions.append(encodings.char_to_token(i, df['answer_start'][i]))\n",
        "        end_positions.append(encodings.char_to_token(i, df['answer_end'][i] - 1))\n",
        "        # if None, the answer passage has been truncated\n",
        "        if start_positions[-1] is None:\n",
        "            start_positions[-1] = tokenizer.model_max_length\n",
        "        if end_positions[-1] is None:\n",
        "            end_positions[-1] = tokenizer.model_max_length\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "add_token_positions(train_encodings, train_df_new)\n",
        "add_token_positions(val_encodings, val_df_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m-M26VSDKKu"
      },
      "source": [
        "Our data is ready. Let's just put it in a PyTorch/TensorFlow dataset so that we can easily use it for training. In\n",
        "PyTorch, we define a custom `Dataset` class. In TensorFlow, we pass a tuple of `(inputs_dict, labels_dict)` to the\n",
        "`from_tensor_slices` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xdH8GgFDKKv"
      },
      "source": [
        "import torch\n",
        "\n",
        "class SquadDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "train_dataset = SquadDataset(train_encodings)\n",
        "val_dataset = SquadDataset(val_encodings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ABf8MYVDKKv"
      },
      "source": [
        "Now we can use a BertForQuestionAnswering for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkDLphZ9DKKv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff2b7768-dbb3-484d-9821-53dc40f86e6c"
      },
      "source": [
        "model = BertForQuestionAnswering.from_pretrained(model_checkpoint)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAlU5A8DPUnR"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjFDPUUsQEAx"
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zawCZxXjDKKx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e39c68b-a514-481f-850f-bd1a2e5a4b68"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "\n",
        "optim = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "for epoch in range(3):\n",
        "  train_losses = []\n",
        "  model.train()\n",
        "  for batch in train_loader:\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    start_positions = batch['start_positions'].to(device)\n",
        "    end_positions = batch['end_positions'].to(device)\n",
        "    # Forward pass\n",
        "    optim.zero_grad()\n",
        "    outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "    loss = outputs[0]\n",
        "    train_losses.append(loss.item())\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optim.step()\n",
        "  print(f\"Train loss: {np.mean(train_losses)}\")\n",
        "  # Validation\n",
        "  model.eval()\n",
        "  val_losses = []\n",
        "  for batch in val_loader:\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    start_positions = batch['start_positions'].to(device)\n",
        "    end_positions = batch['end_positions'].to(device)\n",
        "    with torch.no_grad():\n",
        "      outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "      loss = outputs[0]\n",
        "    val_losses.append(loss.item())\n",
        "  print(f\"Validation loss: {np.mean(val_losses)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train loss: 1.1934588807552198\n",
            "Validation loss: 1.0120271309846784\n",
            "Train loss: 0.8928633790498882\n",
            "Validation loss: 0.8718096106881048\n",
            "Train loss: 0.799929228041795\n",
            "Validation loss: 0.8515546736193866\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCB_BAk8DKKx"
      },
      "source": [
        "<a id='resources'></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc2Koz0V0lJa"
      },
      "source": [
        "torch.save(model.state_dict(), 'bio_bert_model.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJl0TPf90jhX"
      },
      "source": [
        "model.load_state_dict(torch.load('bio_bert_model.bin'), strict=False)\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3r5n2WjZLUu"
      },
      "source": [
        "#OR you can train by Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBRLV24WYs5M"
      },
      "source": [
        "batch_size = 16"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB67nbZEz4fS"
      },
      "source": [
        "args = TrainingArguments(\n",
        "    f\"test-squad\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVi8GaONZBZi"
      },
      "source": [
        "data_collator = default_data_collator"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Nwbnu66z6SI"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_tokenized_dataset,\n",
        "    eval_dataset=val_tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4njWKJ7B0e7b"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pTPxW1ayCnN"
      },
      "source": [
        "trainer.save_model(\"bioasq_with_bert_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI4EcrDCabWz"
      },
      "source": [
        "model.load_state_dict(torch.load('bio_bert_model.bin'), strict=False)\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ShENJnv1CFy"
      },
      "source": [
        "Evaluating our model will require a bit more work, as we will need to map the predictions of our model back to parts of the context. The model itself predicts logits for the start and en position of our answers: if we take a batch from our validation datalaoder, here is the output our model gives us:"
      ]
    }
  ]
}